{"paragraphs":[{"text":"import org.apache.spark.sql.functions._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.expressions.Window\n\nval iowa_df= spark.sqlContext.read.option(\"header\", \"true\").\n    option(\"delimiter\", \",\").option(\"inferSchema\", \"true\").option(\"mode\", \"PERMISSIVE\")\n    .csv(\"/user/maria_dev/final/IowaCity/IowaCityWeather.csv\")\n\nval osh_df = spark.sqlContext.read.option(\"header\", \"true\").\n    option(\"delimiter\", \",\").option(\"inferSchema\", \"true\").option(\"mode\", \"PERMISSIVE\")\n    .csv(\"/user/maria_dev/final/Oshkosh/OshkoshWeather.csv\")\n    \nval osh_df_with_date = osh_df.withColumn(\"date\", to_date(concat_ws(\"-\", $\"Year\", $\"Month\", $\"Day\"), \"yyyy-MM-dd\"))\nval iowa_df_with_date =  iowa_df.withColumn(\"date\", to_date(concat_ws(\"-\", $\"Year\", $\"Month\", $\"Day\"), \"yyyy-MM-dd\"))\n// osh_df_with_date.printSchema()\n// 1\n  // Create a date column\nval osh_df_with_date = osh_df.withColumn(\"date\", to_date(concat_ws(\"-\", $\"Year\", $\"Month\", $\"Day\"), \"yyyy-MM-dd\"))\n\n// Filter for days where the temperature is less than or equal to -10\n// Coldest Day recorded in Wisconsin is -55F on 2/4/1996\n// https://www.weather.gov/mkx/WI_Records\nval cold_days = osh_df_with_date.filter($\"TemperatureF\" <= -10  && $\"TemperatureF\" > -56 ) \n\n// Filter for days where the temperature is greater than or equal to 95\n// Hottest Day recorded in Wisconsin is 114F on 07/13/1936\n// https://www.weather.gov/mkx/WI_Records\nval hot_days = osh_df_with_date.filter($\"TemperatureF\" >= 95 &&  $\"TemperatureF\" < 115)\n\n// Count the number of unique dates in each filtered DataFrame\nval num_cold_days = cold_days.select(\"date\").distinct().count()\nval num_hot_days = hot_days.select(\"date\").distinct().count()\n\nif (num_cold_days > num_hot_days) {\n    println(\"There are more cold days.\")\n    } else if (num_hot_days > num_cold_days) {\n    println(\"There are more hot days.\")\n    } else {\n    println(\"The number of cold and hot days is the same.\")\n}\n","user":"anonymous","dateUpdated":"2024-04-20T21:24:56+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.functions._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.expressions.Window\niowa_df: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 14 more fields]\nosh_df: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 14 more fields]\nosh_df_with_date: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 15 more fields]\niowa_df_with_date: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 15 more fields]\nosh_df_with_date: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 15 more fields]\ncold_days: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Year: int, Month: int ... 15 more fields]\nhot_days: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Year: int, Month: int ... 15 more fields]\nnum_cold_days: Long = 35\nnum_hot_days: Long = 14\nThere are more cold days.\n"}]},"apps":[],"jobName":"paragraph_1713647196192_-606763610","id":"20240420-210636_1463107373","dateCreated":"2024-04-20T21:06:36+0000","dateStarted":"2024-04-20T21:24:56+0000","dateFinished":"2024-04-20T21:29:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3377"},{"text":"import org.apache.spark.sql.functions._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.expressions.Window\n\nval iowa_df= spark.sqlContext.read.option(\"header\", \"true\").\n    option(\"delimiter\", \",\").option(\"inferSchema\", \"true\").option(\"mode\", \"PERMISSIVE\")\n    .csv(\"/user/maria_dev/final/IowaCity/IowaCityWeather.csv\")\n\nval osh_df = spark.sqlContext.read.option(\"header\", \"true\").\n    option(\"delimiter\", \",\").option(\"inferSchema\", \"true\").option(\"mode\", \"PERMISSIVE\")\n    .csv(\"/user/maria_dev/final/Oshkosh/OshkoshWeather.csv\")\n    \nval osh_df_with_date = osh_df.withColumn(\"date\", to_date(concat_ws(\"-\", $\"Year\", $\"Month\", $\"Day\"), \"yyyy-MM-dd\"))\nval iowa_df_with_date =  iowa_df.withColumn(\"date\", to_date(concat_ws(\"-\", $\"Year\", $\"Month\", $\"Day\"), \"yyyy-MM-dd\"))\n\nval monthToSeason = udf((month: Int) => month match {\n  case 12 | 1 | 2 => \"Winter\"\n  case 3 | 4 | 5 => \"Spring\"\n  case 6 | 7 | 8 => \"Summer\"\n  case _ => \"Fall\"\n})\n\n// Add a season column to each DataFrame\nval osh_df_with_season = osh_df_with_date.withColumn(\"Season\", monthToSeason($\"Month\"))\nval iowa_df_with_season = iowa_df_with_date.withColumn(\"Season\", monthToSeason($\"Month\"))\n\n// Calculate the average season temperature for each DataFrame\nval osh_avg_temp_by_season = osh_df_with_season.groupBy(\"Season\").agg(avg(\"TemperatureF\").alias(\"OshAvgTemp\"))\nval iowa_avg_temp_by_season = iowa_df_with_season.groupBy(\"Season\").agg(avg(\"TemperatureF\").alias(\"IowaAvgTemp\"))\n\n// Join the two DataFrames on the season column\nval joined_df = osh_avg_temp_by_season.join(iowa_avg_temp_by_season, Seq(\"Season\"), \"inner\")\n  .withColumnRenamed(\"AvgTemp\", \"OshAvgTemp\")\n  .withColumnRenamed(\"AvgTemp\", \"IowaAvgTemp\")\n\n\n// Calculate the difference of temperatures and display it by season\nval result_df = joined_df.withColumn(\"TempDifference\", $\"OshAvgTemp\" - $\"IowaAvgTemp\")\nresult_df.show()","user":"anonymous","dateUpdated":"2024-04-20T21:29:09+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.functions._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.expressions.Window\niowa_df: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 14 more fields]\nosh_df: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 14 more fields]\nosh_df_with_date: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 15 more fields]\niowa_df_with_date: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 15 more fields]\nmonthToSeason: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(IntegerType)))\nosh_df_with_season: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 16 more fields]\niowa_df_with_season: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 16 more fields]\nosh_avg_temp_by_season: org.apache.spark.sql.DataFrame = [Season: string, OshAvgTemp: double]\niowa_avg_temp_by_season: org.apache.spark.sql.DataFrame = [Season: string, IowaAvgTemp: double]\njoined_df: org.apache.spark.sql.DataFrame = [Season: string, OshAvgTemp: double ... 1 more field]\nresult_df: org.apache.spark.sql.DataFrame = [Season: string, OshAvgTemp: double ... 2 more fields]\n+------+------------------+------------------+-------------------+\n|Season|        OshAvgTemp|       IowaAvgTemp|     TempDifference|\n+------+------------------+------------------+-------------------+\n|Spring|39.996293029357275|28.574087363133415|  11.42220566622386|\n|Summer| 51.86830367790843| 44.43248531818618|  7.435818359722248|\n|  Fall| 43.56719091712797| 43.77507067222465| -0.207879755096684|\n|Winter|0.5780085163510615|10.972379306620574|-10.394370790269512|\n+------+------------------+------------------+-------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1713647224158_1296265747","id":"20240420-210704_1603824439","dateCreated":"2024-04-20T21:07:04+0000","dateStarted":"2024-04-20T21:29:09+0000","dateFinished":"2024-04-20T21:29:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3378"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1713651177800_974253867","id":"20240420-221257_899837438","dateCreated":"2024-04-20T22:12:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3510","text":"import org.apache.spark.sql.functions._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.expressions.Window\n\nval iowa_df= spark.sqlContext.read.option(\"header\", \"true\").\n    option(\"delimiter\", \",\").option(\"inferSchema\", \"true\").option(\"mode\", \"PERMISSIVE\")\n    .csv(\"/user/maria_dev/final/IowaCity/IowaCityWeather.csv\")\n\nval osh_df = spark.sqlContext.read.option(\"header\", \"true\").\n    option(\"delimiter\", \",\").option(\"inferSchema\", \"true\").option(\"mode\", \"PERMISSIVE\")\n    .csv(\"/user/maria_dev/final/Oshkosh/OshkoshWeather.csv\")\n    \nval osh_df_with_date = osh_df.withColumn(\"date\", to_date(concat_ws(\"-\", $\"Year\", $\"Month\", $\"Day\"), \"yyyy-MM-dd\"))\nval iowa_df_with_date =  iowa_df.withColumn(\"date\", to_date(concat_ws(\"-\", $\"Year\", $\"Month\", $\"Day\"), \"yyyy-MM-dd\"))\n\n// Filter the datasets, highest windspeed 135 7/4/1977\n//https://www.fox6now.com/weather/worst-of-the-worst-of-wisconsins-severe-weather\nval filtered_osh_df = osh_df_with_date.filter($\"TemperatureF\" >= -56 && $\"TemperatureF\" <= 115).filter($\"`Wind SpeedMPH`\" >= 0 && $\"`Wind SpeedMPH`\" <= 135)\nval filtered_iowa_df = iowa_df_with_date.filter($\"TemperatureF\" >= -56 && $\"TemperatureF\" <= 115).filter($\"Wind SpeedMPH\" >= 0 && $\"Wind SpeedMPH\" <= 135)\n\n// Concatenate the date and time columns into a single timestamp column\nval osh_df_with_timestamp = filtered_osh_df.withColumn(\"Timestamp\", expr(\"concat(date, ' ', TimeCST)\"))\n  .withColumn(\"Timestamp\", unix_timestamp($\"Timestamp\", \"yyyy-MM-dd hh:mm a\").cast(\"timestamp\"))\nval iowa_df_with_timestamp = filtered_iowa_df.withColumn(\"Timestamp\", expr(\"concat(date, ' ', TimeCST)\"))\n  .withColumn(\"Timestamp\", unix_timestamp($\"Timestamp\", \"yyyy-MM-dd hh:mm a\").cast(\"timestamp\"))\n\n// Convert the timestamp to a numeric value\nval osh_df_with_days = osh_df_with_timestamp.withColumn(\"Days\", datediff(date_trunc(\"day\", $\"Timestamp\"), lit(\"1970-01-01\")))\n\n// Create a 7-day window\nval window = Window.orderBy(col(\"Days\")).rangeBetween(0, 6)\n\n// Calculate the average temperature in each window\nval osh_df_with_avg_temp = osh_df_with_days.withColumn(\"AvgTemp\", avg(\"TemperatureF\").over(window))\n\n// Find the maximum average temperature\nval max_avg_temp = osh_df_with_avg_temp.select(max(\"AvgTemp\")).first().getDouble(0)\n\n// Find the start time of the 7-day period with the maximum average temperature\nval start_time = osh_df_with_avg_temp.filter($\"AvgTemp\" === max_avg_temp).select(min(\"Timestamp\")).first().getTimestamp(0).toLocalDateTime.toLocalDate\n\nprintln(s\"The hottest average temperature over a 7-day period in Oshkosh is $max_avg_temp degrees. The 7-day period starts at $start_time.\")","dateUpdated":"2024-04-20T22:24:04+0000","dateFinished":"2024-04-20T22:24:18+0000","dateStarted":"2024-04-20T22:24:04+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.functions._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.expressions.Window\niowa_df: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 14 more fields]\nosh_df: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 14 more fields]\nosh_df_with_date: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 15 more fields]\niowa_df_with_date: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 15 more fields]\nfiltered_osh_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Year: int, Month: int ... 15 more fields]\nfiltered_iowa_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Year: int, Month: int ... 15 more fields]\nosh_df_with_timestamp: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 16 more fields]\niowa_df_with_timestamp: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 16 more fields]\nosh_df_with_days: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 17 more fields]\nwindow: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@70a11210\nosh_df_with_avg_temp: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 18 more fields]\nmax_avg_temp: Double = 82.54466666666669\nstart_time: java.time.LocalDate = 2012-06-30\nThe hottest average temperature over a 7-day period in Oshkosh is 82.54466666666669 degrees. The 7-day period starts at 2012-06-30.\n"}]}},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1713652296689_700709957","id":"20240420-223136_1061803926","dateCreated":"2024-04-20T22:31:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3849","text":"import org.apache.spark.sql.functions._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.expressions.Window\n\nval iowa_df= spark.sqlContext.read.option(\"header\", \"true\").\n    option(\"delimiter\", \",\").option(\"inferSchema\", \"true\").option(\"mode\", \"PERMISSIVE\")\n    .csv(\"/user/maria_dev/final/IowaCity/IowaCityWeather.csv\")\n\nval osh_df = spark.sqlContext.read.option(\"header\", \"true\").\n    option(\"delimiter\", \",\").option(\"inferSchema\", \"true\").option(\"mode\", \"PERMISSIVE\")\n    .csv(\"/user/maria_dev/final/Oshkosh/OshkoshWeather.csv\")\n    \nval osh_df_with_date = osh_df.withColumn(\"date\", to_date(concat_ws(\"-\", $\"Year\", $\"Month\", $\"Day\"), \"yyyy-MM-dd\"))\nval iowa_df_with_date =  iowa_df.withColumn(\"date\", to_date(concat_ws(\"-\", $\"Year\", $\"Month\", $\"Day\"), \"yyyy-MM-dd\"))\n\n// Filter the datasets, highest windspeed 135 7/4/1977\n//https://www.fox6now.com/weather/worst-of-the-worst-of-wisconsins-severe-weather\nval filtered_osh_df = osh_df_with_date.filter($\"TemperatureF\" >= -56 && $\"TemperatureF\" <= 115).filter($\"`Wind SpeedMPH`\" >= 0 && $\"`Wind SpeedMPH`\" <= 135)\nval filtered_iowa_df = iowa_df_with_date.filter($\"TemperatureF\" >= -56 && $\"TemperatureF\" <= 115).filter($\"Wind SpeedMPH\" >= 0 && $\"Wind SpeedMPH\" <= 135)\n\n// Convert 'TimeCST' to a timestamp format and extract the hour\nval osh_df_with_hour = filtered_osh_df.withColumn(\"Hour\", hour(to_timestamp($\"TimeCST\", \"hh:mm a\")))\n\n// Group by year, month, day, and hour, and calculate the average temperature\nval avg_temp_by_hour = osh_df_with_hour.groupBy($\"Year\", $\"Month\", $\"Day\", $\"Hour\").agg(avg(\"TemperatureF\").as(\"AvgTemp\"))\n\n// Create a window partitioned by year, month, and day, and ordered by average temperature\nval window = Window.partitionBy(\"Year\", \"Month\", \"Day\").orderBy(\"AvgTemp\")\n\n// Rank the average temperatures within each day using dense_rank to allow for ties\nval avg_temp_by_hour_with_rank = avg_temp_by_hour.withColumn(\"Rank\", dense_rank().over(window))\n\n// Filter the DataFrame to include only the rows with the lowest rank\nval min_avg_temp_by_hour = avg_temp_by_hour_with_rank.filter($\"Rank\" === 1)\n\n// Group by 'Hour' and count the number of occurrences of each hour\nval hour_counts = min_avg_temp_by_hour.groupBy(\"Hour\").count()\n\n// Order by count in descending order and take the first row\nval most_frequent_coldest_hour = hour_counts.orderBy(desc(\"count\")).first()\n\nprintln(s\"The most frequent hour to be considered the coldest is ${most_frequent_coldest_hour.getAs[Int](\"Hour\")}, which occurs ${most_frequent_coldest_hour.getAs[Long](\"count\")} times.\")\n","dateUpdated":"2024-04-20T22:59:43+0000","dateFinished":"2024-04-20T23:00:17+0000","dateStarted":"2024-04-20T22:59:43+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.functions._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.expressions.Window\niowa_df: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 14 more fields]\nosh_df: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 14 more fields]\nosh_df_with_date: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 15 more fields]\niowa_df_with_date: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 15 more fields]\nfiltered_osh_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Year: int, Month: int ... 15 more fields]\nfiltered_iowa_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Year: int, Month: int ... 15 more fields]\nosh_df_with_hour: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 16 more fields]\navg_temp_by_hour: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 3 more fields]\nwindow: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@3822a0d9\navg_temp_by_hour_with_rank: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 4 more fields]\nmin_avg_temp_by_hour: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Year: int, Month: int ... 4 more fields]\nhour_counts: org.apache.spark.sql.DataFrame = [Hour: int, count: bigint]\nmost_frequent_coldest_hour: org.apache.spark.sql.Row = [5,1360]\nThe most frequent hour to be considered the coldest is 5, which occurs 1360 times.\n"}]}},{"text":"import org.apache.spark.sql.functions._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.expressions.Window\n\n\nval iowa_df= spark.sqlContext.read.option(\"header\", \"true\").\n    option(\"delimiter\", \",\").option(\"inferSchema\", \"true\").option(\"mode\", \"PERMISSIVE\")\n    .csv(\"/user/maria_dev/final/IowaCity/IowaCityWeather.csv\")\n\nval osh_df = spark.sqlContext.read.option(\"header\", \"true\").\n    option(\"delimiter\", \",\").option(\"inferSchema\", \"true\").option(\"mode\", \"PERMISSIVE\")\n    .csv(\"/user/maria_dev/final/Oshkosh/OshkoshWeather.csv\")\n    \nval osh_df_with_date = osh_df.withColumn(\"date\", to_date(concat_ws(\"-\", $\"Year\", $\"Month\", $\"Day\"), \"yyyy-MM-dd\"))\nval iowa_df_with_date =  iowa_df.withColumn(\"date\", to_date(concat_ws(\"-\", $\"Year\", $\"Month\", $\"Day\"), \"yyyy-MM-dd\"))\n// Filter the datasets\nval filtered_osh_df = osh_df_with_date.filter($\"TemperatureF\" >= -56 && $\"TemperatureF\" <= 115)\nval filtered_iowa_df = iowa_df_with_date.filter($\"TemperatureF\" >= -56 && $\"TemperatureF\" <= 115)\n\n// Concatenate the date and time columns into a single timestamp column\nval osh_df_with_timestamp = filtered_osh_df.withColumn(\"Timestamp\", expr(\"concat(date, ' ', TimeCST)\"))\n  .withColumn(\"Timestamp\", unix_timestamp($\"Timestamp\", \"yyyy-MM-dd hh:mm a\").cast(\"timestamp\"))\nval iowa_df_with_timestamp = filtered_iowa_df.withColumn(\"Timestamp\", expr(\"concat(date, ' ', TimeCST)\"))\n  .withColumn(\"Timestamp\", unix_timestamp($\"Timestamp\", \"yyyy-MM-dd hh:mm a\").cast(\"timestamp\"))\n\n// Filter out the null values in the Timestamp column\nval filtered_osh_df_with_timestamp = osh_df_with_timestamp.filter($\"Timestamp\".isNotNull)\nval filtered_iowa_df_with_timestamp = iowa_df_with_timestamp.filter($\"Timestamp\".isNotNull)\n\n// Convert the Timestamp column to a Unix timestamp\nval osh_df_with_unix_timestamp = filtered_osh_df_with_timestamp.withColumn(\"UnixTime\", unix_timestamp($\"Timestamp\"))\nval iowa_df_with_unix_timestamp = filtered_iowa_df_with_timestamp.withColumn(\"UnixTime\", unix_timestamp($\"Timestamp\"))\n\n// Define the window\nval window = Window.partitionBy(date_format($\"Timestamp\", \"yyyy-MM-dd\")).orderBy($\"UnixTime\").rangeBetween(0, 86400)\n\n// Create a window based on the timestamp\nval osh_df_with_max_min_temp_time =  osh_df_with_unix_timestamp.withColumn(\"MaxTemp\", max(\"TemperatureF\").over(window))\n  .withColumn(\"MinTemp\", min(\"TemperatureF\").over(window))\n  .withColumn(\"MaxTempTime\", max(struct(\"TemperatureF\", \"Timestamp\")).over(window)(\"Timestamp\"))\n  .withColumn(\"MinTempTime\", min(struct(\"TemperatureF\", \"Timestamp\")).over(window)(\"Timestamp\"))\nval iowa_df_with_max_min_temp_time = iowa_df_with_unix_timestamp.withColumn(\"MaxTemp\", max(\"TemperatureF\").over(window))\n  .withColumn(\"MinTemp\", min(\"TemperatureF\").over(window))\n  .withColumn(\"MaxTempTime\", max(struct(\"TemperatureF\", \"Timestamp\")).over(window)(\"Timestamp\"))\n  .withColumn(\"MinTempTime\", min(struct(\"TemperatureF\", \"Timestamp\")).over(window)(\"Timestamp\"))\n\n// Calculate the degrees difference\nval osh_df_with_diff = osh_df_with_max_min_temp_time.withColumn(\"Diff\", $\"MaxTemp\" - $\"MinTemp\")\nval iowa_df_with_diff = iowa_df_with_max_min_temp_time.withColumn(\"Diff\", $\"MaxTemp\" - $\"MinTemp\")\n\n// Find the highest degrees difference and the time when the maximum and minimum temperatures occurred\nval max_diff_osh = osh_df_with_diff.orderBy($\"Diff\".desc).select(\"Diff\", \"MaxTempTime\", \"MinTempTime\").first()\nval max_diff_iowa = iowa_df_with_diff.orderBy($\"Diff\".desc).select(\"Diff\", \"MaxTempTime\", \"MinTempTime\").first()\n\n// Compare the highest degrees difference in Oshkosh and Iowa City and print the city with the higher difference, the temperature difference, and when the maximum and minimum temperature occurred\nif (max_diff_osh.getDouble(0) > max_diff_iowa.getDouble(0)) {\n  println(s\"The city with the highest amount of degrees difference is Oshkosh with a difference of ${max_diff_osh.getDouble(0)} degrees. The maximum temperature occurred at ${max_diff_osh.getTimestamp(1)} and the minimum temperature occurred at ${max_diff_osh.getTimestamp(2)}.\")\n} else if (max_diff_osh.getDouble(0) < max_diff_iowa.getDouble(0)) {\n  println(s\"The city with the highest amount of degrees difference is Iowa City with a difference of ${max_diff_iowa.getDouble(0)} degrees. The maximum temperature occurred at ${max_diff_iowa.getTimestamp(1)} and the minimum temperature occurred at ${max_diff_iowa.getTimestamp(2)}.\")\n} else {\n  println(\"Both Oshkosh and Iowa City have the same highest amount of degrees difference.\")\n}","user":"anonymous","dateUpdated":"2024-04-20T22:25:00+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.functions._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.expressions.Window\niowa_df: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 14 more fields]\nosh_df: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 14 more fields]\nosh_df_with_date: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 15 more fields]\niowa_df_with_date: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 15 more fields]\nfiltered_osh_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Year: int, Month: int ... 15 more fields]\nfiltered_iowa_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Year: int, Month: int ... 15 more fields]\nosh_df_with_timestamp: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 16 more fields]\niowa_df_with_timestamp: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 16 more fields]\nfiltered_osh_df_with_timestamp: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Year: int, Month: int ... 16 more fields]\nfiltered_iowa_df_with_timestamp: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Year: int, Month: int ... 16 more fields]\nosh_df_with_unix_timestamp: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 17 more fields]\niowa_df_with_unix_timestamp: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 17 more fields]\nwindow: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@d14760\nosh_df_with_max_min_temp_time: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 21 more fields]\niowa_df_with_max_min_temp_time: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 21 more fields]\nosh_df_with_diff: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 22 more fields]\niowa_df_with_diff: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 22 more fields]\nmax_diff_osh: org.apache.spark.sql.Row = [41.900000000000006,2003-04-15 16:53:00.0,2003-04-15 23:38:00.0]\nmax_diff_iowa: org.apache.spark.sql.Row = [52.2,2008-12-14 12:29:00.0,2008-12-14 23:44:00.0]\nThe city with the highest amount of degrees difference is Iowa City with a difference of 52.2 degrees. The maximum temperature occurred at 2008-12-14 12:29:00.0 and the minimum temperature occurred at 2008-12-14 23:44:00.0.\n"}]},"apps":[],"jobName":"paragraph_1713647440604_-510055363","id":"20240420-211040_68295229","dateCreated":"2024-04-20T21:10:40+0000","dateStarted":"2024-04-20T21:38:09+0000","dateFinished":"2024-04-20T21:38:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3379"},{"text":"import org.apache.spark.sql.functions._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.expressions.Window\n\nval iowa_df= spark.sqlContext.read.option(\"header\", \"true\").\n    option(\"delimiter\", \",\").option(\"inferSchema\", \"true\").option(\"mode\", \"PERMISSIVE\")\n    .csv(\"/user/maria_dev/final/IowaCity/IowaCityWeather.csv\")\n\nval osh_df = spark.sqlContext.read.option(\"header\", \"true\").\n    option(\"delimiter\", \",\").option(\"inferSchema\", \"true\").option(\"mode\", \"PERMISSIVE\")\n    .csv(\"/user/maria_dev/final/Oshkosh/OshkoshWeather.csv\")\n    \nval osh_df_with_date = osh_df.withColumn(\"date\", to_date(concat_ws(\"-\", $\"Year\", $\"Month\", $\"Day\"), \"yyyy-MM-dd\"))\nval iowa_df_with_date =  iowa_df.withColumn(\"date\", to_date(concat_ws(\"-\", $\"Year\", $\"Month\", $\"Day\"), \"yyyy-MM-dd\"))\n\n// Filter the datasets, highest windspeed 135 7/4/1977\n//https://www.fox6now.com/weather/worst-of-the-worst-of-wisconsins-severe-weather\nval filtered_osh_df = osh_df_with_date.filter($\"TemperatureF\" >= -56 && $\"TemperatureF\" <= 115).filter($\"`Wind SpeedMPH`\" >= 0 && $\"`Wind SpeedMPH`\" <= 135)\nval filtered_iowa_df = iowa_df_with_date.filter($\"TemperatureF\" >= -56 && $\"TemperatureF\" <= 115).filter($\"Wind SpeedMPH\" >= 0 && $\"Wind SpeedMPH\" <= 135)\n\n// Concatenate the date and time columns into a single timestamp column\nval osh_df_with_timestamp = filtered_osh_df.withColumn(\"Timestamp\", expr(\"concat(date, ' ', TimeCST)\"))\n  .withColumn(\"Timestamp\", unix_timestamp($\"Timestamp\", \"yyyy-MM-dd hh:mm a\").cast(\"timestamp\"))\nval iowa_df_with_timestamp = filtered_iowa_df.withColumn(\"Timestamp\", expr(\"concat(date, ' ', TimeCST)\"))\n  .withColumn(\"Timestamp\", unix_timestamp($\"Timestamp\", \"yyyy-MM-dd hh:mm a\").cast(\"timestamp\"))\n\n// Filter out the null values in the Timestamp column\nval filtered_osh_df_with_timestamp = osh_df_with_timestamp.filter($\"Timestamp\".isNotNull)\nval filtered_iowa_df_with_timestamp = iowa_df_with_timestamp.filter($\"Timestamp\".isNotNull)\n\n// Convert the Timestamp column to a Unix timestamp\nval osh_df_with_unix_timestamp = filtered_osh_df_with_timestamp.withColumn(\"UnixTime\", unix_timestamp($\"Timestamp\"))\nval iowa_df_with_unix_timestamp = filtered_iowa_df_with_timestamp.withColumn(\"UnixTime\", unix_timestamp($\"Timestamp\"))\n\n// Extract the month and hour from the timestamp\nval osh_df_with_month_hour = osh_df_with_unix_timestamp.withColumn(\"Month\", month($\"Timestamp\")).withColumn(\"Hour\", hour($\"Timestamp\"))\nval iowa_df_with_month_hour = iowa_df_with_unix_timestamp.withColumn(\"Month\", month($\"Timestamp\")).withColumn(\"Hour\", hour($\"Timestamp\"))\n\n// Group by month and hour, and calculate the average temperature and wind speed\nval osh_avg_df = osh_df_with_month_hour.groupBy(\"Month\", \"Hour\").agg(avg(\"TemperatureF\").alias(\"AvgTemp\"), avg(\"`Wind SpeedMPH`\").alias(\"AvgWindSpeed\"))\nval iowa_avg_df = iowa_df_with_month_hour.groupBy(\"Month\", \"Hour\").agg(avg(\"TemperatureF\").alias(\"AvgTemp\"), avg(\"`Wind SpeedMPH`\").alias(\"AvgWindSpeed\"))\n\n// Compute the absolute difference between the average temperature and 50 degrees\nval osh_diff_df = osh_avg_df.withColumn(\"TempDiff\", abs($\"AvgTemp\" - 50))\nval iowa_diff_df = iowa_avg_df.withColumn(\"TempDiff\", abs($\"AvgTemp\" - 50))\n\n// Define a window partitioned by month and ordered by temperature difference and wind speed\nval window = Window.partitionBy(\"Month\").orderBy(\"TempDiff\", \"AvgWindSpeed\")\n\n// For each month, find the hour with the smallest temperature difference and lowest wind speed\nval osh_best_hour_df = osh_diff_df.withColumn(\"Rank\", rank().over(window)).filter($\"Rank\" === 1).drop(\"Rank\")\nval iowa_best_hour_df = iowa_diff_df.withColumn(\"Rank\", rank().over(window)).filter($\"Rank\" === 1).drop(\"Rank\")\n\n// Add a \"City\" column to each DataFrame\nval osh_best_hour_df_with_city = osh_best_hour_df.withColumn(\"City\", lit(\"Oshkosh\"))\nval iowa_best_hour_df_with_city = iowa_best_hour_df.withColumn(\"City\", lit(\"Iowa City\"))\n\n// Combine the data from both cities into one DataFrame\nval combined_df = osh_best_hour_df_with_city.union(iowa_best_hour_df_with_city)\n\n// Sort the DataFrame by month in ascending order\nval sorted_df = combined_df.sort(\"Month\")\n\n// Rank the rows within each month based on the temperature difference and wind speed\nval window = Window.partitionBy(\"Month\").orderBy(\"TempDiff\", \"AvgWindSpeed\")\nval ranked_df = sorted_df.withColumn(\"Rank\", rank().over(window))\n\n// Filter for the rows with the lowest rank\nval best_hour_df = ranked_df.filter($\"Rank\" === 1)\n\n// Select the desired columns\nval final_df = best_hour_df.select(\"City\", \"Month\", \"Hour\", \"AvgTemp\", \"TempDiff\", \"AvgWindSpeed\")\n\n// Display the final DataFrame\nfinal_df.show()","user":"anonymous","dateUpdated":"2024-04-20T21:39:11+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.functions._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.expressions.Window\niowa_df: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 14 more fields]\nosh_df: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 14 more fields]\nosh_df_with_date: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 15 more fields]\niowa_df_with_date: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 15 more fields]\nfiltered_osh_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Year: int, Month: int ... 15 more fields]\nfiltered_iowa_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Year: int, Month: int ... 15 more fields]\nosh_df_with_timestamp: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 16 more fields]\niowa_df_with_timestamp: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 16 more fields]\nfiltered_osh_df_with_timestamp: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Year: int, Month: int ... 16 more fields]\nfiltered_iowa_df_with_timestamp: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Year: int, Month: int ... 16 more fields]\nosh_df_with_unix_timestamp: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 17 more fields]\niowa_df_with_unix_timestamp: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 17 more fields]\nosh_df_with_month_hour: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 18 more fields]\niowa_df_with_month_hour: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 18 more fields]\nosh_avg_df: org.apache.spark.sql.DataFrame = [Month: int, Hour: int ... 2 more fields]\niowa_avg_df: org.apache.spark.sql.DataFrame = [Month: int, Hour: int ... 2 more fields]\nosh_diff_df: org.apache.spark.sql.DataFrame = [Month: int, Hour: int ... 3 more fields]\niowa_diff_df: org.apache.spark.sql.DataFrame = [Month: int, Hour: int ... 3 more fields]\nwindow: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@5208dfee\nosh_best_hour_df: org.apache.spark.sql.DataFrame = [Month: int, Hour: int ... 3 more fields]\niowa_best_hour_df: org.apache.spark.sql.DataFrame = [Month: int, Hour: int ... 3 more fields]\nosh_best_hour_df_with_city: org.apache.spark.sql.DataFrame = [Month: int, Hour: int ... 4 more fields]\niowa_best_hour_df_with_city: org.apache.spark.sql.DataFrame = [Month: int, Hour: int ... 4 more fields]\ncombined_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Month: int, Hour: int ... 4 more fields]\nsorted_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Month: int, Hour: int ... 4 more fields]\nwindow: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@40efd181\nranked_df: org.apache.spark.sql.DataFrame = [Month: int, Hour: int ... 5 more fields]\nbest_hour_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Month: int, Hour: int ... 5 more fields]\nfinal_df: org.apache.spark.sql.DataFrame = [City: string, Month: int ... 4 more fields]\n+---------+-----+----+------------------+-------------------+------------------+\n|     City|Month|Hour|           AvgTemp|           TempDiff|      AvgWindSpeed|\n+---------+-----+----+------------------+-------------------+------------------+\n|Iowa City|    1|  14|27.544642857142858| 22.455357142857142|11.683184523809526|\n|Iowa City|    2|  14|29.728648648648658| 20.271351351351342| 12.03945945945946|\n|Iowa City|    3|  14|45.703521126760585| 4.2964788732394155|12.244894366197181|\n|Iowa City|    4|   9| 50.20753064798599| 0.2075306479859904|11.583012259194403|\n|  Oshkosh|    5|   5| 50.45555555555553| 0.4555555555555273| 8.936612021857925|\n|  Oshkosh|    6|   4|60.353731343283584| 10.353731343283584| 7.242217484008524|\n|  Oshkosh|    7|   5|  64.3213552361396| 14.321355236139595| 6.845174537987673|\n|  Oshkosh|    8|   5| 63.42389006342498| 13.423890063424977| 6.842071881606762|\n|  Oshkosh|    9|   4| 56.78783505154639|  6.787835051546388|  7.28907216494845|\n|Iowa City|   10|   8| 49.85188118811879|0.14811881188121134| 8.499999999999996|\n|Iowa City|   11|  13|46.846750902527084| 3.1532490974729157| 12.15884476534295|\n|Iowa City|   12|  14| 31.43682634730541|  18.56317365269459|11.058383233532936|\n+---------+-----+----+------------------+-------------------+------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1713648589157_-13485216","id":"20240420-212949_1943358731","dateCreated":"2024-04-20T21:29:49+0000","dateStarted":"2024-04-20T21:39:11+0000","dateFinished":"2024-04-20T21:39:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3380"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1713649151666_-1208306218","id":"20240420-213911_1787699763","dateCreated":"2024-04-20T21:39:11+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:3381"}],"name":"part 1","id":"2JWZUBHXU","angularObjects":{"2CHS8UYQQ:shared_process":[],"2C8A4SZ9T_livy2:shared_process":[],"2CK8A9MEG:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2CKAY1A8Y:shared_process":[],"2CKEKWY8Z:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}